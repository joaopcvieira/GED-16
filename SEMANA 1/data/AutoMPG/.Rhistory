plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.8)
# conferindo os graficos (ajuste perfeito)
plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
# conferindo os graficos (ajuste perfeito)
plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
# Consideremos o conjunto de dados "Boston":
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
cor(Boston)
# Tomemos um subconjunto dos dados
data.3 <- data.frame(X1 = Boston$age, X2 = Boston$dis, Y = Boston$medv)
# Verifica correlacao
plot(data.3)
cor(data.3)
## Constucao de modelos de regressao
# Modelo de regressao com apenas X1
m.1 <- lm(Y ~ X1, data = data.3)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2, data = data.3)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c12 <- lm(Y ~ X1 + X2, data = data.3)
summary(m.c12)
# Modelo completo (inclui X2 e X1, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1, data = data.3)
summary(m.c21)
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
cor(data.3)
plot(X1,X2); points(30,5, col=2, pch =19)
plot(data.3[,1:2]); points(c(30,5), col=2, pch =19)
summary(data.3)
head(data.3)
plot(data.3[,1:2])
plot(X1,X2)
###
### CASO 3: VARIAVEIS EXPLICATIVAS CORRELACIONADAS
###
# Limpa area de trabalho
rm(list=ls())
# Consideremos o conjunto de dados "Boston":
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
cor(Boston)
# Observe a correlacao que existe entre as variaveis "age" e "dis", por exemplo
# Tomemos um subconjunto dos dados
data.3 <- data.frame(X1 = Boston$age, X2 = Boston$dis, Y = Boston$medv)
attach(data.3)
# Verifica correlacao
plot(data.3)
cor(data.3)
## Constucao de modelos de regressao
# Modelo de regressao com apenas X1
m.1 <- lm(Y ~ X1, data = data.3)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2, data = data.3)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c12 <- lm(Y ~ X1 + X2, data = data.3)
summary(m.c12)
# Modelo completo (inclui X2 e X1, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1, data = data.3)
summary(m.c21)
### RESUMO:
#               Estimate Std. Error t value Pr(>|t|)
# m.1
# X1          -0.12316    0.01348  -9.137   <2e-16 ***
# m.2
# X2            1.0916     0.1884   5.795 1.21e-08 ***
# m.c12
# X1           -0.1409     0.0203  -6.941  1.2e-11 ***
# X2           -0.3170     0.2714  -1.168    0.243
## Efeitos da multicolinearidade
# --------
# EFEITO 1:
# Quando as variaveis explicativas sao correlacionadas, os coeficientes
# estimados dependem das variaveis ja incluidas no modelo.
# --------
# Observe os valores obtidos para os coeficientes de X1 e X2 nos modelos
# m.1, m.2 e m.12:
### RESUMO:
# Variaveis no modelo      b1          b2
# X1                       -0.12316    ----
# X2                       ----         1.0916
# X1, X2                   -0.1409     -0.3170
# Quando apenas X2 (distancia) eh incluida no modelo,
# podemos inferir que cada acrescimo unitario na distancia provoca um acrescimo no valor
# mediano dos imoveis de aprox. 1.09.Por outro lado, se X1 e X2 sao ambas incluidas
# no modelo, para cada acrescimo unitario na distancia o valor mediano dos imoveis
# cai aprox. 0.32.
# As duas analises de regressao nos levam a conclusoes cientificas conflitantes!
# --------
# EFEITO 2:
# Quando as variaveis explicativas sao correlacionadas, a precisao dos coeficientes
# estimados diminui conforme mais variaveis explicativas sao incluidas no modelo.
# --------
### RESUMO:
# Variaveis no modelo      se(b1)      se(b2)
# X1                       0.01348     ----
# X2                       ----        0.1884
# X1, X2                   0.0203      0.2714
# O erro padrao para b2, no modelo que inclui ambas X1 e X2 eh aprox. 1.5 vezes
# maior (0.271/0.188) do que o erro padrao para o coeficiente estimado do modelo m.2.
# O mesmo ocorre para os erros padrao dos coeficientes estimados de X1 nos modelos
# m.1 e m.12.
# Erros padrao aumentados levam a intervalos de confianca mais largos e estimativas
# menos precisas para os coeficientes, portanto.
# --------
# EFEITO 3:
# Quando as variaveis explicativas sao correlacionadas, as contribuicoes marginais de
# cada variavel explicativa para a reducao da soma dos quadrados varia, dependendo
# das variaveis ja incluidas no modelo.
# --------
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
# --------
### RESUMO:
# Variaveis no modelo      SSR
# X1                       6070
# X2                       2668
# X1, X2                   99    :SS(X2|X1)
# X2, X1                   3501  :SS(X1|X2)
# Aqui o modelo m.2 sugere que a inclusao de X2 reduz SS substancialmente (2668),
# enquanto que o modelo m.c12 sugere que a inclusao de X2 nao ajuda tanto (99) a
# reduzir a soma dos quadrados, depois de considerar o efeito de X1.
# A maior parte da variabilidade observada nos precos medianos de imoveis
# que eh explicada por distancia (X2) ja foi explicada por idade (X1), ja que
# as duas variaveis sao correlacionadas. O entendimento e' semelhante se as
# variaveis sao incluidas no modelo na ordem inversa.
# Observe que a contribuicao de X1 e' maior, pois esta var. explicativa
# tem relacao linear mais forte com a resposta que X2:
# cor(Y, X1) = -0.38; cor(Y, X2) = 0.25
# --------
# EFEITO 4:
# Quando as variaveis explicativas sao correlacionadas, testes de hipoteses para
# beta_k = 0 podem levar a conclusoes diferentes, dependendo das variaveis ja incluidas
# no modelo (consequencia dos efeitos anteriores).
# --------
### RESUMO:
# Variaveis no modelo      Pr(>|t|) beta_1    Pr(>|t|) beta_2
# X1                       <2e-16 ***         ----
# X2                       ----               1.21e-08 ***
# X1, X2                   1.2e-11 ***        0.243
# Existe evidencia suficiente para crer que o preco mediano dos imoveis
# eh linearmente relacionado a X1 (idade), levando ou nao em conta X2 (distancia).
# Por outro lado, nao podemos concluir que exista relacao estatisticamente
# linear entre Y (preco) e X2 (distancia), apos levar em conta a idade do imovel.
# Esta conclusao parece contraditoria com o resultado do modelo levando em conta apenas
# X2 (em que ha significancia estatististica). No entanto, o que estes resultados mostram
# eh que, uma vez que X1 (a idade do imovel) eh considerada, X2 (a distancia) nao explica muito
# da variabilidade observada nos precos dos imoveis.
# --------
# EFEITO 5:
# A presenca de multicolinearidade entre as variaveis explicativas nao impede que boas (precisas)
# previsoes sejam obtidas para a variavel de resposta, desde que seja considerado
# o escopo do modelo.
# --------
# Suponha o seguinte ponto X1 (age) = 30 e X2 (dist) = 5, que pertence ao escopo do modelo.
summary(data.3)
plot(X1,X2); points(c(30,5), col=2, pch =19)
plot(X1,X2)
points(c(30,5)
, col=2, pch =19)
X1
X2
plot(X2~X1)
points(c(30,5), col=2, pch =19)
summary(data.3)
plot(data.3[,-3])
plot(data.3[,-3], xlim = c(0, 110), ylim=c(0,13))
plot(X1)
plot(data.3)
plot(data.3[,-3], xlim = c(0, 110), ylim=c(0,13))
new.obs <- c(30, 5)
points(new.obs, col=2, pch =19)
summary(data.3)
plot(X2~X1)
new.obs <- c(30, 5)
points(new.obs, col=2, pch =19)
plot(X2~X1, cex = 0.7)
new.obs <- c(30, 5)
points(new.obs, col=2, pch =19)
summary(data.3)
plot(X2~X1, cex = 0.7)
points(x=30, y=5, col=2, pch =19)
new.x1 <- data.frame(X1=30)
predict(m.1, newdata = new.x1, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.1, newdata = new.x1, interval="prediction", level=0.95)
new.x2 <- data.frame(X2=5)
predict(m.2, newdata = new.x2, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.2, newdata = new.x2, interval="prediction", level=0.95)
new.x12 <- data.frame(X1=30, X2=5)
predict(m.c12, newdata = new.x12, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.c12, newdata = new.x12, interval="prediction", level=0.95)
new.x12 <- data.frame(X1=30, X2=5)
predict(m.c12, newdata = new.x12, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.c12, newdata = new.x12, interval="prediction", level=0.95)
# IP.95% (30, 5): (10,78; 44,38)
Y  <- c(42, 39, 48, 51, 49, 53, 61, 60)
X1 <- c(4,4,4,4,6,6,6,6)
X2 <- c(2,2,3,3,2,2,3,3)
data.1 <- data.frame(Y= Y, X1=X1, X2=X2)
data.1
cor(data.1)
plot(data.1)
m.1 <- lm(Y ~ X1)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1)
summary(m.c21)
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
# Limpa area de trabalho
rm(list=ls())
# Seja agora o seguinte conjunto de dados:
Y  <- c(23, 83, 63, 103)
X1 <- c(2,8,6, 10)
X2 <- c(6, 9, 8, 10)
data.2 <- data.frame(Y= Y, X1=X1, X2=X2)
plot(data.2)
cor(data.2) # de fato
mc <- lm(Y~X1+X2, data = data.3)
mc <- lm(Y~X1+X2, data = data.2)
summary(mc)
sum(resid2.A <- (Y - Yhat.A(X1, X2))^2)
# o ajuste e' perfeito
Yhat.A <- function(X1, X2) {
return(-87 + X1 + 18 * X2)
}
# Vamos calcular a soma dos residuos quadraticos:
sum(resid2.A <- (Y - Yhat.A(X1, X2))^2)
# o ajuste e' perfeito
# O mesmo foi feito pelo aluno "B", que encontrou o seguinte modelo ajustado:
Yhat.B <- function(X1, X2) {
return(-7 + 9 * X1 + 2 *X2)
}
# Vamos calcular a soma dos residuos quadraticos:
sum(resid.B <- (Y - Yhat.B(X1, X2))^2)
# aqui o ajuste tambem e' perfeito
# conferindo os graficos (ajuste perfeito)
plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
###
### CASO 3: VARIAVEIS EXPLICATIVAS CORRELACIONADAS
###
# Limpa area de trabalho
rm(list=ls())
# Consideremos o conjunto de dados "Boston":
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
cor(Boston)
# Observe a correlacao que existe entre as variaveis "age" e "dis", por exemplo
# Tomemos um subconjunto dos dados
data.3 <- data.frame(X1 = Boston$age, X2 = Boston$dis, Y = Boston$medv)
attach(data.3)
# Verifica correlacao
plot(data.3)
cor(data.3)
## Constucao de modelos de regressao
# Modelo de regressao com apenas X1
m.1 <- lm(Y ~ X1, data = data.3)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2, data = data.3)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c12 <- lm(Y ~ X1 + X2, data = data.3)
summary(m.c12)
# Modelo completo (inclui X2 e X1, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1, data = data.3)
summary(m.c21)
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
2668+3501+36547
cor(data.3)
summary(data.3)
plot(X2~X1, cex = 0.7)
points(x=30, y=5, col=2, pch =19)
new.x1 <- data.frame(X1=30)
predict(m.1, newdata = new.x1, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.1, newdata = new.x1, interval="prediction", level=0.95)
# IP.95% (30): (10,48; 44,08)
new.x2 <- data.frame(X2=5)
predict(m.2, newdata = new.x2, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.2, newdata = new.x2, interval="prediction", level=0.95)
new.x12 <- data.frame(X1=30, X2=5)
predict(m.c12, newdata = new.x12, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.c12, newdata = new.x12, interval="prediction", level=0.95)
Y  <- c(42, 39, 48, 51, 49, 53, 61, 60)
X1 <- c(4,4,4,4,6,6,6,6)
X2 <- c(2,2,3,3,2,2,3,3)
data.1 <- data.frame(Y= Y, X1=X1, X2=X2)
data.1
# Verifica que X1 e X2 tem correlacao nula
cor(data.1)
plot(data.1)
m.1 <- lm(Y ~ X1)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2)
summary(m.2)
m.c12 <- lm(Y ~ X1 + X2)
summary(m.c12)
m.c21 <- lm(Y ~ X2 + X1)
summary(m.c21)
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
# Seja agora o seguinte conjunto de dados:
Y  <- c(23, 83, 63, 103)
X1 <- c(2,8,6, 10)
X2 <- c(6, 9, 8, 10)
data.2 <- data.frame(Y= Y, X1=X1, X2=X2)
plot(data.2)
# Y aparentemente tambem tem relacao deterministica com as var. explicativas
cor(data.2) # de fato
Yhat.A <- function(X1, X2) {
return(-87 + X1 + 18 * X2)
}
sum(resid2.A <- (Y - Yhat.A(X1, X2))^2)
# o ajuste e' perfeito
# O mesmo foi feito pelo aluno "B", que encontrou o seguinte modelo ajustado:
Yhat.B <- function(X1, X2) {
return(-7 + 9 * X1 + 2 *X2)
}
sum(resid.B <- (Y - Yhat.B(X1, X2))^2)
# conferindo os graficos (ajuste perfeito)
plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
# Limpa area de trabalho
rm(list=ls())
# Consideremos o conjunto de dados "Boston":
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
cor(Boston)
# Tomemos um subconjunto dos dados
data.3 <- data.frame(X1 = Boston$age, X2 = Boston$dis, Y = Boston$medv)
attach(data.3)
# Verifica correlacao
plot(data.3)
cor(data.3)
# Modelo de regressao com apenas X1
m.1 <- lm(Y ~ X1, data = data.3)
summary(m.1)
m.2 <- lm(Y ~ X2, data = data.3)
summary(m.2)
m.c12 <- lm(Y ~ X1 + X2, data = data.3)
summary(m.c12)
# Modelo completo (inclui X2 e X1, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1, data = data.3)
summary(m.c21)
install.packages("car")
qt(0.05, 22)
# Limpa area de trabalho
rm(list=ls())
Y  <- c(42, 39, 48, 51, 49, 53, 61, 60)
X1 <- c(4,4,4,4,6,6,6,6)
X2 <- c(2,2,3,3,2,2,3,3)
data.1 <- data.frame(Y= Y, X1=X1, X2=X2)
data.1
cor(data.1)
plot(data.1)
m.1 <- lm(Y ~ X1)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c12 <- lm(Y ~ X1 + X2)
summary(m.c12)
m.c21 <- lm(Y ~ X2 + X1)
summary(m.c21)
# ANOVA
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
# Limpa area de trabalho
rm(list=ls())
# Seja agora o seguinte conjunto de dados:
Y  <- c(23, 83, 63, 103)
X1 <- c(2, 8, 6, 10)
X2 <- c(6, 9, 8, 10)
data.2 <- data.frame(Y= Y, X1=X1, X2=X2)
data.2
plot(data.2)
cor(data.2) # de fato
Yhat.A <- function(X1, X2) {
return(-87 + X1 + 18 * X2)
}
sum(resid2.A <- (Y - Yhat.A(X1, X2))^2)
# O mesmo foi feito pelo aluno "B", que encontrou o seguinte modelo ajustado:
Yhat.B <- function(X1, X2) {
return(-7 + 9 * X1 + 2 *X2)
}
# Vamos calcular a soma dos residuos quadraticos:
sum(resid.B <- (Y - Yhat.B(X1, X2))^2)
# conferindo os graficos (ajuste perfeito)
plot(Y~Yhat.A(X1,X2))
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
plot(Y~Yhat.A(X1,X2), xlab = "Y.hat")
points(Y~Yhat.B(X1,X2), col=2, pch=19, cex=0.5)
# Limpa area de trabalho
rm(list=ls())
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
library(MASS)
data.3 <- Boston  # 1 resposta: medv, 13 var. explicativas
help(Boston)
cor(Boston)
cor(Boston)
# Tomemos um subconjunto dos dados
data.3 <- data.frame(X1 = Boston$age, X2 = Boston$dis, Y = Boston$medv)
attach(data.3)
# Verifica correlacao
plot(data.3)
cor(data.3)
m.1 <- lm(Y ~ X1, data = data.3)
summary(m.1)
# Modelo de regressao com apenas X2
m.2 <- lm(Y ~ X2, data = data.3)
summary(m.2)
# Modelo completo (inclui X1 e X2, nesta ordem)
m.c12 <- lm(Y ~ X1 + X2, data = data.3)
summary(m.c12)
# Modelo completo (inclui X2 e X1, nesta ordem)
m.c21 <- lm(Y ~ X2 + X1, data = data.3)
summary(m.c21)
anova(m.1)
anova(m.2)
anova(m.c12)
anova(m.c21)
cor(data.3)
summary(data.3)
plot(X2~X1, cex = 0.7)
points(x=30, y=5, col=2, pch =19)
new.x1 <- data.frame(X1=30)
predict(m.1, newdata = new.x1, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.1, newdata = new.x1, interval="prediction", level=0.95)
new.x2 <- data.frame(X2=5)
predict(m.2, newdata = new.x2, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.2, newdata = new.x2, interval="prediction", level=0.95)
new.x12 <- data.frame(X1=30, X2=5)
predict(m.c12, newdata = new.x12, interval="confidence", level=0.95, se.fit = TRUE)
predict(m.c12, newdata = new.x12, interval="prediction", level=0.95)
# IC.95% (30, 5): (26.22; 28.95)
# IP.95% (30, 5): (10,78; 44,38)
###
### PARTE 1: Analise Exploratoria de Dados
###
# Limpa area de trabalho
rm(list=ls())
# Carrega os dados em uma tabela
my.data <- read.table("auto-mpg.data", header = FALSE)
setwd("~/Dropbox/ITA/Ensino/_Disciplinas/MOQ14:PO216/20181_Denise/S07 [2018-04-04]/Rcode/AutoMPG")
###
### PARTE 1: Analise Exploratoria de Dados
###
# Limpa area de trabalho
rm(list=ls())
# Carrega os dados em uma tabela
my.data <- read.table("auto-mpg.data", header = FALSE)
# Verifica uma pequena parte da tabela
head(my.data)
# Nomeia variaveis
colnames(my.data) <- c("mpg", "cylinder", "displacement", "hp",
"weight", "acceleration", "modelyear", "origin", "carname")
attach(my.data) # Para podermos utilizar os nomes das variaveis diretamente
summary(my.data)
cylinder  <- as.factor(cylinder)
hp        <- as.numeric(hp)
modelyear <- as.factor(modelyear)
origin    <- as.factor(origin)
carname   <- as.character(carname)
my.data <- data.frame(mpg, cylinder, displacement, hp, weight,
acceleration, modelyear, origin, carname)
# Gera grafico de dispersao
plot(my.data)
# Matriz de correlacao
cor (my.data)
data.reduc <- my.data[,c(1,3,4,5,6)]
cor(data.reduc)
plot(data.reduc)
# Constroi modelo de regressao linear
mod <- lm(mpg ~ displacement + hp + weight + acceleration,
data = my.data)
# Visualiza modelo ajustado
summary(mod)
setwd("~/Dropbox/ITA/Ensino/_Disciplinas/MOQ14:PO216/20181_Denise/S07 [2018-04-05]/Rcode/AutoMPG")
dim(data.reduc)
